{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Analysis of Jobs that Requested and Used VRAM](#toc0_)\n",
    "This notebook generates the analysis for jobs that requested some VRAM and run on partitions that their type is GPU and some GPU VRAM is used. It looks at these jobs, corresponding users, and PI groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Analysis of Jobs that Requested and Used VRAM](#toc1_)    \n",
    "  - [Setup](#toc1_1_)    \n",
    "  - [Data Digestion and Preprocessing](#toc1_2_)    \n",
    "  - [Narrowing Dataset to Relevant Partition](#toc1_3_)    \n",
    "  - [Generate Baseline Metrics](#toc1_4_)    \n",
    "  - [Job-Level Analysis](#toc1_5_)    \n",
    "      - [Find most inefficient jobs with no VRAM constraints based on `requested_vram_efficiency_score`](#toc1_5_1_1_)    \n",
    "    - [Generate all hoarding analysis metrics for jobs:](#toc1_5_2_)    \n",
    "      - [Find most inefficient jobs hoarding node RAM based on `ram_hoarding_fraction_diff`](#toc1_5_2_1_)    \n",
    "      - [Find most inefficient jobs hoarding CPU cores based on `core_hoarding_fraction_diff`](#toc1_5_2_2_)    \n",
    "  - [User-Level Analysis](#toc1_6_)    \n",
    "    - [Find Inefficient Users based on `requested_vram_efficiency_score`](#toc1_6_1_)    \n",
    "    - [Generate all hoarding analysis metrics for users:](#toc1_6_2_)    \n",
    "      - [Find most inefficient users hoarding node RAM based on `expected_value_ram_hoarding_fraction_diff`](#toc1_6_2_1_)    \n",
    "      - [Find most inefficient users hoarding CPU cores based on `expected_value_core_hoarding_fraction_diff`](#toc1_6_2_2_)    \n",
    "  - [PI Group Analysis](#toc1_7_)    \n",
    "      - [Find Inefficient PIs based on `avg_requested_vram_efficiency_score`](#toc1_7_1_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Jupyter server should be run at the notebook directory, so the output of the following cell would be the project root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = str(Path.cwd().resolve().parent.parent)\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules before executing code (set this up BEFORE imports)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add project root to sys.path for module imports\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.analysis import ResourceHoarding\n",
    "from src.analysis import efficiency_analysis as ea\n",
    "from src.visualization import JobsWithMetricsVisualizer, UsersWithMetricsVisualizer, PIGroupsWithMetricsVisualizer\n",
    "from src.config.enum_constants import ResourceHoardingDataFrameNameEnum\n",
    "from src.config.paths import PI_GROUPS_VISUALIZATION_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Data Digestion and Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the jobs DataFrame from DuckDB\n",
    "preprocessed_jobs_df = ea.load_preprocessed_jobs_dataframe_from_duckdb(\n",
    "    db_path=Path(project_root) / \"data/slurm_data.db\",\n",
    "    table_name=\"Jobs\",\n",
    ")\n",
    "display(preprocessed_jobs_df.head(10))\n",
    "print(preprocessed_jobs_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Narrowing Dataset to Relevant Partition](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ResourceHoarding(jobs_df=preprocessed_jobs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jobs = analyzer.filter_jobs_for_analysis(\n",
    "    gpu_mem_usage_filter={\"min\": 0, \"inclusive\": False}, requested_vram_filter={\"min\": 0, \"inclusive\": False}\n",
    ")\n",
    "filtered_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Generate Baseline Metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = analyzer.calculate_all_efficiency_metrics(filtered_jobs)\n",
    "\n",
    "jobs_with_metrics = metrics_dict[\"jobs_with_efficiency_metrics\"]\n",
    "users_with_metrics = metrics_dict[\"users_with_efficiency_metrics\"]\n",
    "pi_accounts_with_metrics = metrics_dict[\"pi_accounts_with_efficiency_metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Job-Level Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Display the DataFrame\n",
    "display(jobs_with_metrics.head(10))\n",
    "# To revert to default settings (optional)\n",
    "pd.reset_option(\"display.max_columns\")\n",
    "\n",
    "print(f\"Jobs found: {len(jobs_with_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### <a id='toc1_5_1_1_'></a>[Find most inefficient jobs with no VRAM constraints based on `requested_vram_efficiency_score`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_jobs_vram_hours = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.JOBS,\n",
    "    sorting_key=\"requested_vram_efficiency_score\",\n",
    "    ascending=True,  # Sort by requested_vram_efficiency_score in ascending order\n",
    "    filter_criteria={\n",
    "        \"alloc_vram_efficiency_score\": {\"max\": -10, \"inclusive\": True},  # score threshold\n",
    "    },\n",
    ")\n",
    "# Display top inefficient users by requested VRAM efficiency score\n",
    "print(\"\\nTop inefficient Jobs by Requested VRAM Efficiency Score:\")\n",
    "display(inefficient_jobs_vram_hours.head(10))\n",
    "\n",
    "# Plot top inefficient jobs by requested VRAM efficiency score, with VRAM-hours as labels\n",
    "jobs_with_metrics_visualizer = JobsWithMetricsVisualizer(inefficient_jobs_vram_hours.head(10))\n",
    "jobs_with_metrics_visualizer.visualize(\n",
    "    column=\"requested_vram_efficiency_score\", bar_label_columns=[\"vram_hours\", \"allocated_vram\"], figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Generate all hoarding analysis metrics for jobs:](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_hoarding_jobs = analyzer.calculate_node_resource_hoarding_for_jobs(filtered_jobs)\n",
    "\n",
    "# Set option to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Display the DataFrame\n",
    "display(memory_hoarding_jobs.head(10))\n",
    "# To revert to default settings (optional)\n",
    "pd.reset_option(\"display.max_columns\")\n",
    "\n",
    "print(f\"Jobs found: {len(memory_hoarding_jobs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### <a id='toc1_5_2_1_'></a>[Find most inefficient jobs hoarding node RAM based on `ram_hoarding_fraction_diff`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_jobs_hoarding_ram = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.JOBS_WITH_RESOURCE_HOARDING_METRICS,\n",
    "    sorting_key=\"ram_hoarding_fraction_diff\",\n",
    "    ascending=False,  # Sort in descending order\n",
    "    filter_criteria={\"ram_hoarding_fraction_diff\": {\"min\": 0, \"inclusive\": True}},\n",
    ")\n",
    "# Display top inefficient users by RAM hoarding fraction\n",
    "print(\"\\nTop inefficient Jobs by RAM hoarding fraction:\")\n",
    "display(inefficient_jobs_hoarding_ram.head(10))\n",
    "\n",
    "# Plot top inefficient jobs by RAM hoarding fraction, with RAM hoarding fraction as labels\n",
    "jobs_with_metrics_visualizer = JobsWithMetricsVisualizer(inefficient_jobs_hoarding_ram.head(10))\n",
    "jobs_with_metrics_visualizer.visualize(\n",
    "    column=\"ram_hoarding_fraction_diff\",\n",
    "    bar_label_columns=[\"cpu_mem_efficiency\", \"alloc_vram_efficiency\"],\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### <a id='toc1_5_2_2_'></a>[Find most inefficient jobs hoarding CPU cores based on `core_hoarding_fraction_diff`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_jobs_hoarding_cpu_cores = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.JOBS_WITH_RESOURCE_HOARDING_METRICS,\n",
    "    sorting_key=\"core_hoarding_fraction_diff\",\n",
    "    ascending=False,  # Sort in descending order\n",
    "    filter_criteria={\"core_hoarding_fraction_diff\": {\"min\": 0, \"inclusive\": True}},\n",
    ")\n",
    "# Display top inefficient users by CPU core hoarding fraction\n",
    "print(\"\\nTop inefficient Jobs by CPU core hoarding fraction:\")\n",
    "display(inefficient_jobs_hoarding_cpu_cores.head(10))\n",
    "\n",
    "# Plot top inefficient jobs by CPU core hoarding fraction, with CPU core hoarding fraction as labels\n",
    "jobs_with_metrics_visualizer = JobsWithMetricsVisualizer(inefficient_jobs_hoarding_cpu_cores.head(10))\n",
    "jobs_with_metrics_visualizer.visualize(\n",
    "    column=\"core_hoarding_fraction_diff\",\n",
    "    bar_label_columns=[\"ram_hoarding_fraction_diff\", \"alloc_vram_efficiency\"],\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[User-Level Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_1_'></a>[Find Inefficient Users based on `requested_vram_efficiency_score`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_users_avg_req_vram_eff_score = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.USERS,\n",
    "    sorting_key=\"avg_requested_vram_efficiency_score\",\n",
    "    ascending=True,  # Sort by avg_requested_vram_efficiency_score in ascending order\n",
    "    filter_criteria={\n",
    "        \"avg_requested_vram_efficiency_score\": {\"max\": -10, \"inclusive\": True},  # score threshold\n",
    "        \"job_count\": {\"min\": 5, \"inclusive\": True},  # minimum job count threshold\n",
    "    },\n",
    ")\n",
    "# Display top inefficient users by avg_requested_vram_efficiency_score\n",
    "print(\"\\nTop inefficient users by Avg Requested VRAM Efficiency Score:\")\n",
    "display(inefficient_users_avg_req_vram_eff_score.head(10))\n",
    "print(len(inefficient_users_avg_req_vram_eff_score), \"inefficient users found.\")\n",
    "\n",
    "# Plot top inefficient users by Avg Requested VRAM Efficiency Score, with avg_requested_vram_efficiency_score as labels\n",
    "users_with_metrics_visualizer = UsersWithMetricsVisualizer(inefficient_users_avg_req_vram_eff_score.head(10))\n",
    "users_with_metrics_visualizer.visualize(\n",
    "    column=\"avg_requested_vram_efficiency_score\", bar_label_columns=[\"vram_hours\", \"job_count\"], figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Avg Requested VRAM Efficiency Score (actual values; all are <= 0)\n",
    "# We keep scores as-is (negative or zero) and construct bins that respect the skew while\n",
    "# still giving higher resolution near zero using log-spaced absolute values mapped back to negatives.\n",
    "scores = inefficient_users_avg_req_vram_eff_score[\"avg_requested_vram_efficiency_score\"].dropna()\n",
    "print(len(scores), \"scores found for plotting.\")\n",
    "if scores.empty:\n",
    "    print(\"No scores to plot.\")\n",
    "else:\n",
    "    # If all scores are exactly zero, a histogram is not informative\n",
    "    if (scores != 0).sum() == 0:\n",
    "        print(\"All scores are zero; histogram not informative.\")\n",
    "    else:\n",
    "        import numpy as np\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "        # Separate negatives (expected) from zeros\n",
    "        neg_scores = scores[scores < 0]\n",
    "        zero_scores = scores[scores == 0]\n",
    "\n",
    "        min_abs = None  # track smallest non-zero absolute value for symlog threshold\n",
    "\n",
    "        # Build bins: if we have negative values, create log-spaced absolute edges then map back\n",
    "        if not neg_scores.empty:\n",
    "            n_bins = 100\n",
    "            min_abs = neg_scores.abs().min()\n",
    "            max_abs = neg_scores.abs().max()\n",
    "            if min_abs == max_abs:\n",
    "                # Degenerate case: all negative values identical -> fall back to linear bins\n",
    "                bins = np.linspace(neg_scores.min(), 0, 20)\n",
    "            else:\n",
    "                abs_edges = np.logspace(np.log10(min_abs), np.log10(max_abs), n_bins)\n",
    "                # Convert absolute edges to negative edges (descending), then append 0 as the last edge\n",
    "                neg_edges = -abs_edges[::-1]\n",
    "                bins = np.unique(np.concatenate([neg_edges, [0]]))  # ensure strictly increasing\n",
    "        else:\n",
    "            # No negative values (only zeros) already handled earlier; fallback just in case\n",
    "            bins = 3\n",
    "\n",
    "        sns.histplot(scores, bins=bins, color=\"#1f77b4\", ax=ax)\n",
    "        ax.set_xlabel(\"Avg Requested VRAM Efficiency Score (<= 0)\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(\"Distribution of Avg Requested VRAM Efficiency Scores (Actual Values, Log X)\")\n",
    "\n",
    "        # Apply symmetrical log scale to x-axis to compress the long negative tail while keeping zero.\n",
    "        # linthresh defines the range around zero that stays linear; choose smallest non-zero magnitude.\n",
    "        if min_abs is not None and min_abs > 0:\n",
    "            linthresh = min_abs\n",
    "        else:\n",
    "            linthresh = 1e-6  # fallback small threshold\n",
    "        ax.set_xscale(\"symlog\", linthresh=linthresh, linscale=1.0, base=10)\n",
    "\n",
    "        # Annotation: counts (negative & zero) and total\n",
    "        neg_count = (scores < 0).sum()\n",
    "        zero_count = (scores == 0).sum()\n",
    "        total = len(scores)\n",
    "        ax.text(\n",
    "            0.98,\n",
    "            0.95,\n",
    "            (f\"Counts:\\nNegative: {neg_count}\\nZero: {zero_count}\\n# of Users: {total}\"),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"right\",\n",
    "            va=\"top\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9),\n",
    "        )\n",
    "\n",
    "        # Cumulative distribution (CDF) over actual score values\n",
    "        counts, bin_edges = np.histogram(scores, bins=bins)\n",
    "        cdf = np.cumsum(counts) / counts.sum()\n",
    "        mids = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(mids, cdf, color=\"crimson\", marker=\"o\", linestyle=\"-\", linewidth=1, markersize=3)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_ylabel(\"Cumulative Fraction\", color=\"crimson\")\n",
    "        ax2.tick_params(axis=\"y\", colors=\"crimson\")\n",
    "\n",
    "        # Ensure x-axis spans to (slightly) include zero for clarity\n",
    "        left, right = ax.get_xlim()\n",
    "        if right < 0:\n",
    "            ax.set_xlim(left, 0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Notes:\n",
    "# - We plot the actual (negative/zero) scores instead of absolute values.\n",
    "# - symlog x-scale provides a log-like compression for large negative magnitudes while keeping zero.\n",
    "# - linthresh picks the smallest non-zero magnitude so near-zero structure is visible.\n",
    "# - CDF is computed over actual values to show accumulation from most negative toward zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### <a id='toc1_6_2_'></a>[Generate all hoarding analysis metrics for users:](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_hoarding_users = analyzer.calculate_node_resource_hoarding_for_users(filtered_jobs)\n",
    "display(memory_hoarding_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_1_'></a>[Find most inefficient users hoarding node RAM based on `expected_value_ram_hoarding_fraction_diff`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_users_hoarding_ram = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.USERS_WITH_RESOURCE_HOARDING_METRICS,\n",
    "    sorting_key=\"expected_value_ram_hoarding_fraction_diff\",\n",
    "    ascending=False,  # Sort in descending order\n",
    "    filter_criteria={\"expected_value_ram_hoarding_fraction_diff\": {\"min\": 0, \"inclusive\": True}},\n",
    ")\n",
    "# Display top inefficient users by RAM hoarding fraction\n",
    "\n",
    "print(\"\\nTop inefficient Users by RAM hoarding fraction:\")\n",
    "display(inefficient_users_hoarding_ram.head(10))\n",
    "\n",
    "# Plot top inefficient users by RAM hoarding fraction, with RAM hoarding fraction as labels\n",
    "users_with_metrics_visualizer = UsersWithMetricsVisualizer(inefficient_users_hoarding_ram.head(10))\n",
    "users_with_metrics_visualizer.visualize(\n",
    "    column=\"expected_value_ram_hoarding_fraction_diff\",\n",
    "    bar_label_columns=[\n",
    "        \"expected_value_alloc_vram_efficiency\",\n",
    "    ],\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### <a id='toc1_6_2_2_'></a>[Find most inefficient users hoarding CPU cores based on `expected_value_core_hoarding_fraction_diff`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_users_hoarding_cpu_cores = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.USERS_WITH_RESOURCE_HOARDING_METRICS,\n",
    "    sorting_key=\"expected_value_core_hoarding_fraction_diff\",\n",
    "    ascending=False,  # Sort in descending order\n",
    "    filter_criteria={\"expected_value_core_hoarding_fraction_diff\": {\"min\": 0, \"inclusive\": True}},\n",
    ")\n",
    "# Display top inefficient users by CPU core hoarding fraction\n",
    "\n",
    "print(\"\\nTop inefficient Users by CPU core hoarding fraction:\")\n",
    "display(inefficient_users_hoarding_cpu_cores.head(10))\n",
    "\n",
    "# Plot top inefficient users by CPU core hoarding fraction, with CPU core hoarding fraction as labels\n",
    "users_with_metrics_visualizer = UsersWithMetricsVisualizer(inefficient_users_hoarding_cpu_cores.head(10))\n",
    "users_with_metrics_visualizer.visualize(\n",
    "    column=\"expected_value_core_hoarding_fraction_diff\",\n",
    "    bar_label_columns=[\n",
    "        \"expected_value_alloc_vram_efficiency\",\n",
    "    ],\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[PI Group Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_accounts_with_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### <a id='toc1_7_1_1_'></a>[Find Inefficient PIs based on `avg_requested_vram_efficiency_score`](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inefficient_pis_avg_req_vram_eff_score = analyzer.sort_and_filter_records_with_metrics(\n",
    "    metrics_df_name_enum=ResourceHoardingDataFrameNameEnum.PI_GROUPS,\n",
    "    sorting_key=\"avg_requested_vram_efficiency_score\",\n",
    "    ascending=True,  # more negative first\n",
    "    filter_criteria={\n",
    "        \"avg_requested_vram_efficiency_score\": {\"max\": -10, \"inclusive\": True},\n",
    "        \"job_count\": {\"min\": 5, \"inclusive\": True},  # Minimum number of jobs to consider a PI account\n",
    "    },\n",
    ")\n",
    "# Display top inefficient PI groups by Avg Requested VRAM Efficiency Score\n",
    "print(\"\\nTop inefficient PI Groups by Avg Requested VRAM Efficiency Score:\")\n",
    "display(inefficient_pis_avg_req_vram_eff_score.head(10))\n",
    "\n",
    "pis_with_metrics_visualizer = PIGroupsWithMetricsVisualizer(inefficient_pis_avg_req_vram_eff_score.head(10))\n",
    "pis_with_metrics_visualizer.visualize(\n",
    "    output_dir_path=PI_GROUPS_VISUALIZATION_DATA_DIR,\n",
    "    column=\"avg_requested_vram_efficiency_score\",\n",
    "    bar_label_columns=[\"pi_acc_job_hours\", \"pi_acc_vram_hours\"],\n",
    "    figsize=(10, 6),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
