{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "    print(f\"Added {parent_dir} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e8bea-5de8-430c-be4a-5f9c268cdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and load the dataframe with job information\n",
    "from scripts.gpu_metrics import GPUMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import duckdb\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_palette(\"muted\")\n",
    "# Filter out jobs less than 10 minutes\n",
    "metrics = GPUMetrics(min_elapsed=600, metricsfile=\"../data/slurm_data_small.db\")\n",
    "df = metrics.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad34b63-0a18-4f3c-9787-7223355a42b8",
   "metadata": {},
   "source": [
    "First we take a look at average and median queue wait times for jobs, based on how much GPU VRam they request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac722fed-aeee-4dc5-8322-784842481ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"queued_seconds\"] = df[\"Queued\"].apply(lambda x: x.total_seconds())\n",
    "df[\"total_seconds\"] = df[\"Elapsed\"] + df[\"queued_seconds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11480943-6fa5-4685-9c5a-5c1213e46387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_queued(stat=\"Mean\"):\n",
    "    \"\"\"Plot Queue statistics\"\"\"\n",
    "    gb = df[~df[\"IsArray\"] & (df[\"GPUs\"] == 1)].groupby([\"requested_vram\"])[\n",
    "        [\"queued_seconds\", \"Elapsed\", \"total_seconds\", \"TimeLimit\"]\n",
    "    ]\n",
    "    if stat == \"Mean\":\n",
    "        gb = gb.mean()\n",
    "    elif stat == \"Median\":\n",
    "        gb = gb.median()\n",
    "\n",
    "    plotting_df = gb / 3600\n",
    "    plotting_df[\"TimeLimit\"] *= 60\n",
    "    plotting_df.rename(columns={\"queued_seconds\": \"Queued\", \"total_seconds\": \"Total\", \"Elapsed\": \"Ran\"}).plot.bar()\n",
    "    plt.title(stat + \" Queue and Run times\")\n",
    "    plt.xlabel(\"Requested VRAM\")\n",
    "    plt.ylabel(\"Hours\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_queued()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feec163-848c-4510-942e-b0150ed8e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_relplot():\n",
    "    \"\"\"Plot relationship between timelimit and queue time\"\"\"\n",
    "    mask = ~df[\"IsArray\"] & (df[\"GPUs\"] == 1) & (df[\"Partition\"] == \"gpu\")\n",
    "    plotting_df = pd.DataFrame({\n",
    "        \"Queued Hours\": df[\"queued_seconds\"][mask] / 3600,\n",
    "        \"Requested Hours\": df[\"TimeLimit\"][mask] / 60,\n",
    "        \"Requested VRAM (G)\": df[\"requested_vram\"][mask],\n",
    "    }).clip(0, 100)\n",
    "    sns.relplot(data=plotting_df, x=\"Requested Hours\", y=\"Queued Hours\", hue=\"Requested VRAM (G)\")\n",
    "    # plt.plot(\"Queued vs Requested Hours on Unity\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "do_relplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b09a34-fb43-4be2-9585-21eac1bca739",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queued(stat=\"Median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ce0eb-358d-4ae2-8b40-9be99da18535",
   "metadata": {},
   "source": [
    "Next we examine VRAM usage levels for all jobs, jobs with no specific VRAM request, and for jobs that request the largest GPU possible (80G) of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30cf45-a64f-496b-94e1-4dcfd744540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpu_metrics import vram_cutoffs, vram_labels\n",
    "\n",
    "\n",
    "def efficiency_plot(constrs=None, title=\"Used GPU VRAM by GPU Compute Hours\"):\n",
    "    if constrs is None:\n",
    "        constrs = []\n",
    "    if len(constrs):\n",
    "        where = \"where \" + (\" and \".join(constrs))\n",
    "    else:\n",
    "        where = \"\"\n",
    "\n",
    "    where = \"where \" + (\" and \".join(constrs)) if constrs else \"\"\n",
    "    filtered_df = duckdb.query(\n",
    "        \"select GPUs, GPUMemUsage, Elapsed, requested_vram, IsArray, Elapsed*GPUs/3600 as gpu_hours  from df \" + where\n",
    "    ).df()\n",
    "    filtered_df[\"used_vram\"] = pd.cut(filtered_df[\"GPUMemUsage\"] / 2**30, labels=vram_labels, bins=vram_cutoffs)\n",
    "    # filtered_df.groupby([\"requested_vram\", \"IsArray\"])[\"gpu_hours\"].sum()\n",
    "    filtered_df.loc[(filtered_df[\"used_vram\"] == 12), \"used_vram\"] = 11\n",
    "    filtered_df.loc[(filtered_df[\"used_vram\"] == 16), \"used_vram\"] = 23\n",
    "    tot_hours = filtered_df.groupby([\"used_vram\"], observed=False)[\"gpu_hours\"].sum().reset_index()\n",
    "    tot_hours = tot_hours[tot_hours[\"gpu_hours\"] > 0]\n",
    "    tot_hours.plot.pie(\n",
    "        figsize=(9, 9),\n",
    "        legend=False,\n",
    "        y=\"gpu_hours\",\n",
    "        labels=[f\"{i}G\" for i in tot_hours[\"used_vram\"]],\n",
    "        autopct=\"%1.1f%%\",\n",
    "    )\n",
    "    # tot_hours.plot.pie()\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "efficiency_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7098f34-ebc5-44ad-8453-831fb3e4133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot(constrs=[\"requested_vram=0\"], title=\"Used GPU VRam by GPU Compute Hours (no VRAM constraint)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91abc3-8f36-44fa-895b-e0e91b2e0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot(\n",
    "    constrs=[\"requested_vram>=80\", \"partition!='superpod-a100'\"],\n",
    "    title=\"Used GPU VRam by GPU Compute Hours (requested VRAM>=80G)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30120541-d6e4-4e3f-97ea-83d54a315f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
