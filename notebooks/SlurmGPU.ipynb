{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Slurm GPU Job Analytics MVP\n",
    "\n",
    "This notebook is an MVP analytics script, originally provided by project lead Benjamin, to help the team get started with GPU job data exploration and visualization on the Unity cluster.\n",
    "\n",
    "It demonstrates how to load, filter, and visualize job data, focusing on queue times, VRAM usage, and efficiency metrics for GPU workloads.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and load the dataframe with job information\n",
    "from gpu_metrics import GPUMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import duckdb\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_palette(\"muted\")\n",
    "# Filter out jobs less than 10 minutes\n",
    "metrics = GPUMetrics(min_elapsed=600)\n",
    "df = metrics.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First we take a look at average and median queue wait times for jobs, based on how much GPU VRam they request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"queued_seconds\"] = df[\"Queued\"].apply(lambda x: x.total_seconds())\n",
    "df[\"total_seconds\"] = df[\"Elapsed\"] + df[\"queued_seconds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_queued(stat=\"Mean\"):\n",
    "    \"\"\"Plot Queue statistics\"\"\"\n",
    "    gb = df[~df[\"IsArray\"] & (df[\"GPUs\"] == 1)].groupby([\"requested_vram\"])[\n",
    "        [\"queued_seconds\", \"Elapsed\", \"total_seconds\", \"TimeLimit\"]\n",
    "    ]\n",
    "    if stat == \"Mean\":\n",
    "        gb = gb.mean()\n",
    "    elif stat == \"Median\":\n",
    "        gb = gb.median()\n",
    "\n",
    "    plotting_df = gb / 3600\n",
    "    plotting_df[\"TimeLimit\"] *= 60\n",
    "    plotting_df.rename(columns={\"queued_seconds\": \"Queued\", \"total_seconds\": \"Total\", \"Elapsed\": \"Ran\"}).plot.bar()\n",
    "    plt.title(stat + \" Queue and Run times\")\n",
    "    plt.xlabel(\"Requested VRAM\")\n",
    "    plt.ylabel(\"Hours\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_queued()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_relplot():\n",
    "    \"\"\"Plot relationship between timelimit and queue time\"\"\"\n",
    "    mask = ~df[\"IsArray\"] & (df[\"GPUs\"] == 1) & (df[\"Partition\"] == \"gpu\")\n",
    "    plotting_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Queued Hours\": df[\"queued_seconds\"][mask] / 3600,\n",
    "            \"Requested Hours\": df[\"TimeLimit\"][mask] / 60,\n",
    "            \"Requested VRAM (G)\": df[\"requested_vram\"][mask],\n",
    "        }\n",
    "    ).clip(0, 100)\n",
    "    sns.relplot(data=plotting_df, x=\"Requested Hours\", y=\"Queued Hours\", hue=\"Requested VRAM (G)\")\n",
    "    # plt.plot(\"Queued vs Requested Hours on Unity\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "do_relplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_queued(stat=\"Median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Next we examine VRAM usage levels for all jobs, jobs with no specific VRAM request, and for jobs that request the largest GPU possible (80G) of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpu_metrics import vram_cutoffs, vram_labels\n",
    "\n",
    "\n",
    "def efficiency_plot(constrs=None, title=\"Used GPU VRAM by GPU Compute Hours\"):\n",
    "    if constrs is None:\n",
    "        constrs = []\n",
    "    if len(constrs):\n",
    "        where = \"where \" + (\" and \".join(constrs))\n",
    "    else:\n",
    "        where = \"\"\n",
    "\n",
    "    where = \"where  + \"(\" and \".join(constrs)) if constrs else \"\"\n",
    "    filtered_df = duckdb.query(\n",
    "        \"select GPUs, GPUMemUsage, Elapsed, requested_vram, IsArray, Elapsed*GPUs/3600 as gpu_hours  from df \" + where\n",
    "    ).df()\n",
    "    filtered_df[\"used_vram\"] = pd.cut(filtered_df[\"GPUMemUsage\"] / 2**30, labels=vram_labels, bins=vram_cutoffs)\n",
    "    # filtered_df.groupby([\"requested_vram\", \"IsArray\"])[\"gpu_hours\"].sum()\n",
    "    filtered_df.loc[(filtered_df[\"used_vram\"] == 12), \"used_vram\"] = 11\n",
    "    filtered_df.loc[(filtered_df[\"used_vram\"] == 16), \"used_vram\"] = 23\n",
    "    tot_hours = filtered_df.groupby([\"used_vram\"], observed=False)[\"gpu_hours\"].sum().reset_index()\n",
    "    tot_hours = tot_hours[tot_hours[\"gpu_hours\"] > 0]\n",
    "    tot_hours.plot.pie(\n",
    "        figsize=(9, 9),\n",
    "        legend=False,\n",
    "        y=\"gpu_hours\",\n",
    "        labels=[f\"{i}G\" for i in tot_hours[\"used_vram\"]],\n",
    "        autopct=\"%1.1f%%\",\n",
    "    )\n",
    "    # tot_hours.plot.pie()\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "efficiency_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot(constrs=[\"requested_vram=0\"], title=\"Used GPU VRam by GPU Compute Hours (no VRAM constraint)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_plot(\n",
    "    constrs=[\"requested_vram>=80\", \"partition!='superpod-a100'\"],\n",
    "    title=\"Used GPU VRam by GPU Compute Hours (requested VRAM>=80G)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Duck DB",
   "language": "python",
   "name": "duckdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
